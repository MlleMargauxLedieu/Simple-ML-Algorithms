{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set up our variahbles declaration and imports\n",
    "\n",
    "Coding: utf-8\n",
    "Dataset of annoted tweet can be found : http://help.sentiment140.com/for-students/\n",
    " \n",
    "0 -> negative\n",
    "2 -> neutral\n",
    "4 -> positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please make sure that you have downloaded -  nltk.download('vader_lexicon')\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from sklearn import metrics\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from random import shuffle\n",
    "import random\n",
    "import numpy \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare X and Y variables, percentage split, hyperparameters and constraints \n",
    "\n",
    "X1 = [] #data\n",
    "Y1 = [] #label\n",
    "X2 = [] #data\n",
    "Y2 = [] #label\n",
    "X3 = [] #data\n",
    "Y3 = [] #label\n",
    "X4 = [] #data\n",
    "Y4 = [] #label\n",
    "Y4_vader = [] #label\n",
    "\n",
    "percentagesplit=0.3\n",
    "SVM= LinearSVC() #define type of SVM that will be used\n",
    "sentdict=0 #restrict features to a particular feature list: 0=No, 1=Vader\n",
    "ngram_min=1 #for anything greater than 1 sentdict must equal 0\n",
    "ngram_max=3\n",
    " \n",
    "#Variables for data distribution calculations\n",
    "num_4 = 0\n",
    "num_2 = 0\n",
    "num_0 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "dataset1='testdata.manual.2009.06.14.csv'\n",
    "dataset2='training.1600000.processed.noemoticon.csv'\n",
    "dataset3='VadaDatasetTweets.txt'\n",
    "dataset4='TweetsAirline.csv' #airline tweets\n",
    "#dataset5=GOPtwitterdatabase an option if we want it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next we load in our training and testing data, exploring various datasets for train/testing to select the best model parameters.\n",
    "# We will classify 0 as negative, 2 as neutral and 4 as positve.\n",
    "# In[2]:\n",
    "\n",
    "#loading and Saving tweets from databases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset1 - 500 tweets 182 pos, 139 neut, 177 neg\n",
    "file = open(dataset1)\n",
    "lines = csv.reader(file)\n",
    "\n",
    "for line in lines :\n",
    "    X1.append(line[5])\n",
    "    Y1.append(int(line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training.1600000.processed.noemoticon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3c0e54c8594e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Dataset2 - 1600 tweets 800,000 pos and 800,00 negative since it is so big we take a subset of everything to not manage too much data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latin-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#lines = csv.reader(file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.1600000.processed.noemoticon.csv'"
     ]
    }
   ],
   "source": [
    "#Dataset2 - 1600 tweets 800,000 pos and 800,00 negative since it is so big we take a subset of everything to not manage too much data\n",
    "file = open(dataset2,encoding=\"latin-1\")\n",
    "#lines = csv.reader(file)\n",
    "lines = file.readlines()\n",
    "shuffle(lines)\n",
    "\n",
    "for line in lines[:2000]:\n",
    "    line = line.split(',')\n",
    "    X2.append(line[5])\n",
    "    Y2.append(int(line[0][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset3 - VADER dataset \n",
    "file = open(dataset3)\n",
    "\n",
    "line = file.readline()\n",
    "\n",
    "while line :\n",
    "    \n",
    "    line = line.split('\\t')\n",
    "    \n",
    "    X3.append(line[2])\n",
    "    \n",
    "#    Thresholds selected upon analysis of the tweets and splits 2042 positve, 1227 neutral, 931 negative\n",
    "    \n",
    "    if float((line[1]))>=0.8: #positive sentiment\n",
    "        Y3.append(4)\n",
    "    elif float(line[1])<=-0.8: #negativesentiment\n",
    "        Y3.append(0)\n",
    "    else: \n",
    "        Y3.append(2)\n",
    "    \n",
    "    \n",
    "    line = file.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset4 - 14641 tweets 2363 pos, 3100 neut, 9178 neg\n",
    "file = open(dataset4,encoding=\"utf8\")\n",
    "lines = csv.reader(file)\n",
    "\n",
    "for line in lines:\n",
    "    X4.append(line[10])\n",
    "    Y4_vader.append(line[2])\n",
    "    \n",
    "    if line[1]=='positive': #positive sentiment\n",
    "        Y4.append(4)\n",
    "    elif line[1]=='negative': #negativesentiment\n",
    "        Y4.append(0)\n",
    "    else: \n",
    "        Y4.append(2)\n",
    "   #as data is skewed lets extract 3000 negatives\n",
    "idx_of_rand_negatives =  random.sample( [i for i, x in enumerate(Y4) if x == 0],3000)\n",
    "idx_of_pos =  random.sample( [i for i, x in enumerate(Y4) if x == 4],2363)\n",
    "idx_of_neutr=  random.sample( [i for i, x in enumerate(Y4) if x == 2],3000)\n",
    "idx_2_keep = idx_of_rand_negatives+idx_of_pos+idx_of_neutr\n",
    "X4= [X4[i] for i in idx_2_keep]\n",
    "Y4 = [Y4[i] for i in idx_2_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we select how we construct our training/test data base by adding the previous datasets    \n",
    "#X = X1 + X2 + X3 + X4\n",
    "#Y = Y1 + Y2 + Y3 + Y4\n",
    "\n",
    "##Or choose only 1 particular dataset\n",
    "X = X1\n",
    "Y = Y1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate X_train, X_test by splitting X\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=percentagesplit,random_state=12)\n",
    "#Another way X_train=X1+X2, X_test=X3+X4\n",
    "\n",
    "#Counting data distribution\n",
    "for line in X_train:\n",
    "    if line == 4:\n",
    "        num_4 += 1\n",
    "    elif line== 2:\n",
    "        num_2 += 1\n",
    "    elif line == 0:\n",
    "        num_0 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentdict==0: #Do not restrict the vectorizer to specific features\n",
    "    sentiment_dict=None\n",
    "elif sentdict==1: #Use the vader dataset to extract significant feature words from the data\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict=SentimentIntensityAnalyzer.make_lex_dict(analyzer).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the scikit-learn tf-idf to tranfrom the data https://en.wikipedia.org/wiki/Tf%E2%80%93idf and http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting than might be better that the bag of word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####===================================\n",
    "##   Different vectorizers (count, binary, tfidf)\n",
    "###================================\n",
    "#Create different vectorizers \n",
    "\n",
    "\n",
    "vectorizer1 = CountVectorizer(dtype=int,ngram_range=(ngram_min, ngram_max),lowercase=True,max_df=0.8, min_df=1,stop_words='english', vocabulary= sentiment_dict, binary=True)\n",
    "\n",
    "vectorizer2=CountVectorizer(dtype=int,ngram_range=(ngram_min, ngram_max),lowercase=True,max_df=0.8, min_df=1,stop_words='english', vocabulary= sentiment_dict) #\n",
    "\n",
    "vectorizer3=TfidfVectorizer(ngram_range=(ngram_min, ngram_max), min_df=1, #Suppress word that appear in less than 10 docs\n",
    "                             max_df = 0.8, #Suppress word that appear in more than 80% of the doc\n",
    "                             sublinear_tf=True,\n",
    "                             smooth_idf = True, # adds “1” to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:\n",
    "\n",
    "use_idf=True, dtype=int,stop_words='english', vocabulary= sentiment_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data\n",
    "#Binary\n",
    "train_vectors1 = vectorizer1.fit_transform(X_train) \n",
    "test_vectors1 = vectorizer1.transform(X_test)\n",
    "\n",
    "#Count\n",
    "train_vectors2 = vectorizer2.fit_transform(X_train) \n",
    "test_vectors2 = vectorizer2.transform(X_test)\n",
    "\n",
    "#TfIdf\n",
    "train_vectors3= vectorizer3.fit_transform(X_train) \n",
    "test_vectors3 = vectorizer3.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM Classifiers.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsOneClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## =============================================================================\n",
    "## #SVM Model 1 and 2 with different vectorizers\n",
    "## =============================================================================\n",
    "\n",
    "print('Training SVM Classifiers.....')\n",
    "#models _1: binary vectorizers, _2: Count _3:Tfidf\n",
    "\n",
    "model1_1 = OneVsRestClassifier(SVM) \n",
    "model1_2 = OneVsRestClassifier(SVM)\n",
    "model1_3 = OneVsRestClassifier(SVM)\n",
    "model2_1 = OneVsOneClassifier(SVM)\n",
    "model2_2 = OneVsOneClassifier(SVM)\n",
    "model2_3 = OneVsOneClassifier(SVM)\n",
    "\n",
    "model1_1.fit(train_vectors1,Y_train)\n",
    "model2_1.fit(train_vectors1,Y_train)\n",
    "\n",
    "model1_2.fit(train_vectors2,Y_train)\n",
    "model2_2.fit(train_vectors2,Y_train)\n",
    "\n",
    "model1_3.fit(train_vectors3,Y_train)\n",
    "model2_3.fit(train_vectors3,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_scores1_1 [0.60231884 0.64011638 0.69648161 0.65112152 0.66766046]\n",
      "cv_scores1_3 [0.61487165 0.64011638 0.71160028 0.66461185 0.66766046]\n",
      "cv_scores1_3 [0.72589744 0.61527778 0.7387538  0.66335697 0.62554847]\n",
      "cv_scores2_1 [0.65703239 0.60669533 0.69648161 0.63687899 0.66902321]\n",
      "cv_scores2_2 [0.64417922 0.65072345 0.69648161 0.65107598 0.66902321]\n",
      "cv_scores2_3  [0.72732943 0.57390873 0.74868806 0.66335697 0.61783121]\n"
     ]
    }
   ],
   "source": [
    "#Run cross-validation to test split for each model vectorizer pair\n",
    "cv_scores1_1 = cross_val_score(model1_1, train_vectors1, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scores1_2= cross_val_score(model1_2, train_vectors2, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scores1_3 = cross_val_score(model1_3, train_vectors3, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scores2_1 = cross_val_score(model2_1, train_vectors1, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scores2_2= cross_val_score(model2_2, train_vectors2, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scores2_3 = cross_val_score(model2_3, train_vectors3, Y_train, cv=5,scoring='f1_macro')\n",
    "\n",
    "print('cv_scores1_1', cv_scores1_1)\n",
    "print('cv_scores1_3', cv_scores1_2)\n",
    "print('cv_scores1_3', cv_scores1_3)\n",
    "print('cv_scores2_1', cv_scores2_1)\n",
    "print('cv_scores2_2', cv_scores2_2)\n",
    "print('cv_scores2_3 ', cv_scores2_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVM Accuracy.....\n"
     ]
    }
   ],
   "source": [
    "#Test the data\n",
    "print('Testing SVM Accuracy.....')\n",
    "\n",
    "resultsY_11 = model1_1.predict(test_vectors1)\n",
    "resultsY_21 = model2_1.predict(test_vectors1)\n",
    "\n",
    "resultsY_12 = model1_2.predict(test_vectors2)\n",
    "resultsY_22 = model2_2.predict(test_vectors2)\n",
    "\n",
    "resultsY_13 = model1_3.predict(test_vectors3)\n",
    "resultsY_23 = model2_3.predict(test_vectors3)\n",
    "\n",
    "precision11, recall11, score11, support11 = metrics.precision_recall_fscore_support(Y_test,resultsY_11 )\n",
    "precision21, recall21, score21, support21 = metrics.precision_recall_fscore_support(Y_test,resultsY_21 )\n",
    "accuracy11 = metrics.accuracy_score(Y_test,resultsY_11)\n",
    "accuracy21 = metrics.accuracy_score(Y_test,resultsY_21)\n",
    "confusionmatrix11=metrics.confusion_matrix(Y_test,resultsY_11)\n",
    "confusionmatrix21=metrics.confusion_matrix(Y_test,resultsY_21)\n",
    "\n",
    "precision12, recall12, score12, support12 = metrics.precision_recall_fscore_support(Y_test,resultsY_12 )\n",
    "precision22, recall22, score22, support22 = metrics.precision_recall_fscore_support(Y_test,resultsY_22 )\n",
    "accuracy12 = metrics.accuracy_score(Y_test,resultsY_12)\n",
    "accuracy22 = metrics.accuracy_score(Y_test,resultsY_22)\n",
    "confusionmatrix12=metrics.confusion_matrix(Y_test,resultsY_12)\n",
    "confusionmatrix22=metrics.confusion_matrix(Y_test,resultsY_22)\n",
    "\n",
    "precision13, recall13, score13, support13 = metrics.precision_recall_fscore_support(Y_test,resultsY_13 )\n",
    "precision23, recall23, score23, support23 = metrics.precision_recall_fscore_support(Y_test,resultsY_23 )\n",
    "accuracy13 = metrics.accuracy_score(Y_test,resultsY_13)\n",
    "accuracy23 = metrics.accuracy_score(Y_test,resultsY_23)\n",
    "confusionmatrix13=metrics.confusion_matrix(Y_test,resultsY_13)\n",
    "confusionmatrix23=metrics.confusion_matrix(Y_test,resultsY_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "RESULTS - Model 1 - OneVsRestClassifier(LinearSVC())\n",
      "__________________________________________\n",
      "BINARY\n",
      "Accuracy 0.7466666666666667\n",
      "F1_score [0.79279279 0.70886076 0.72727273]\n",
      "COUNT\n",
      "Accuracy 0.7466666666666667\n",
      "F1_score [0.78571429 0.71794872 0.72727273]\n",
      "TFIDF\n",
      "Accuracy 0.7466666666666667\n",
      "F1_score [0.80373832 0.69135802 0.73214286]\n",
      "__________________________________________\n",
      "RESULTS - Model 2 - OneVsOneClassifier(LinearSVC())\n",
      "__________________________________________\n",
      "BINARY\n",
      "Accuracy 0.7466666666666667\n",
      "F1_score [0.77777778 0.70886076 0.74336283]\n",
      "COUNT\n",
      "Accuracy 0.7333333333333333\n",
      "F1_score [0.76363636 0.70886076 0.72072072]\n",
      "TFIDF\n",
      "Accuracy 0.74\n",
      "F1_score [0.77358491 0.7        0.73684211]\n",
      "__________________________________________\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('__________________________________________') \n",
    "print('RESULTS - Model 1 - OneVsRestClassifier(LinearSVC())') \n",
    "print('__________________________________________') \n",
    "\n",
    "print ('BINARY')\n",
    "print('Accuracy', accuracy11)\n",
    "print('F1_score', score11)\n",
    "print ('COUNT')\n",
    "print('Accuracy', accuracy12)\n",
    "print('F1_score', score12)\n",
    "print ('TFIDF')\n",
    "print('Accuracy', accuracy13)\n",
    "print('F1_score', score13)\n",
    "print('__________________________________________') \n",
    "#print('__________________________________________') \n",
    "print('RESULTS - Model 2 - OneVsOneClassifier(LinearSVC())') \n",
    "print('__________________________________________') \n",
    "print ('BINARY')\n",
    "print('Accuracy', accuracy21)\n",
    "print('F1_score', score21)\n",
    "print ('COUNT')\n",
    "print('Accuracy', accuracy22)\n",
    "print('F1_score', score22)\n",
    "print ('TFIDF')\n",
    "print('Accuracy', accuracy23)\n",
    "print('F1_score', score23)\n",
    "print('__________________________________________') \n",
    "print('__________________________________________') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifiers.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## =============================================================================\n",
    "## #DecisionTreeClassifier with different vectorizers - 2 criterion  : “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "## =============================================================================\n",
    "\n",
    "print('Training Decision Tree Classifiers.....')\n",
    "#tree _1: binary vectorizers, _2: Count _3:Tfidf\n",
    "\n",
    "\n",
    "treeGini_1 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "treeGini_2 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "treeGini_3 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "treeEntropy_1 = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "treeEntropy_2 = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "treeEntropy_3 = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "treeGini_1.fit(train_vectors1,Y_train)\n",
    "treeGini_2.fit(train_vectors2,Y_train)\n",
    "treeGini_3.fit(train_vectors3,Y_train)\n",
    "\n",
    "\n",
    "treeEntropy_1.fit(train_vectors1,Y_train)\n",
    "treeEntropy_2.fit(train_vectors2,Y_train)\n",
    "treeEntropy_3.fit(train_vectors3,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_scoresGini_1 [ 0.57527039  0.55400895  0.66763652  0.52622937  0.6233781 ]\n",
      "cv_scoresGini_3 [ 0.61606542  0.55594683  0.6513977   0.59165875  0.56554829]\n",
      "cv_scoresGini_3 [ 0.65595252  0.49850075  0.66492375  0.50068027  0.6173981 ]\n",
      "cv_scoresEntropy_1 [ 0.57535311  0.57273264  0.68078747  0.51654662  0.61133474]\n",
      "cv_scoresEntropy_2 [ 0.57272781  0.53705017  0.55308837  0.56261023  0.63861428]\n",
      "cv_scoresEntropy_3  [ 0.64157509  0.50152555  0.60306554  0.56574074  0.56846845]\n"
     ]
    }
   ],
   "source": [
    "#Run cross-validation to test split for each model vectorizer pair\n",
    "cv_scoresGini_1 = cross_val_score(treeGini_1, train_vectors1, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresGini_2= cross_val_score(treeGini_2, train_vectors2, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresGini_3 = cross_val_score(treeGini_3, train_vectors3, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresEntropy_1 = cross_val_score(treeEntropy_1, train_vectors1, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresEntropy_2= cross_val_score(treeEntropy_2, train_vectors2, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresEntropy_3 = cross_val_score(treeEntropy_3, train_vectors3, Y_train, cv=5,scoring='f1_macro')\n",
    "\n",
    "print('cv_scoresGini_1', cv_scoresGini_1)\n",
    "print('cv_scoresGini_3', cv_scoresGini_2)\n",
    "print('cv_scoresGini_3', cv_scoresGini_3)\n",
    "print('cv_scoresEntropy_1', cv_scoresEntropy_1)\n",
    "print('cv_scoresEntropy_2', cv_scoresEntropy_2)\n",
    "print('cv_scoresEntropy_3 ', cv_scoresEntropy_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Decision Tree Classifier Accuracy.....\n"
     ]
    }
   ],
   "source": [
    "#Test the data\n",
    "print('Testing Decision Tree Classifier Accuracy.....')\n",
    "\n",
    "resultsY_11 = treeGini_1.predict(test_vectors1)\n",
    "resultsY_21 = treeEntropy_1.predict(test_vectors1)\n",
    "\n",
    "resultsY_12 = treeGini_2.predict(test_vectors2)\n",
    "resultsY_22 = treeEntropy_2.predict(test_vectors2)\n",
    "\n",
    "resultsY_13 = treeGini_3.predict(test_vectors3)\n",
    "resultsY_23 = treeEntropy_3.predict(test_vectors3)\n",
    "\n",
    "precision11, recall11, score11, support11 = metrics.precision_recall_fscore_support(Y_test,resultsY_11 )\n",
    "precision21, recall21, score21, support21 = metrics.precision_recall_fscore_support(Y_test,resultsY_21 )\n",
    "accuracy11 = metrics.accuracy_score(Y_test,resultsY_11)\n",
    "accuracy21 = metrics.accuracy_score(Y_test,resultsY_21)\n",
    "confusionmatrix11=metrics.confusion_matrix(Y_test,resultsY_11)\n",
    "confusionmatrix21=metrics.confusion_matrix(Y_test,resultsY_21)\n",
    "\n",
    "precision12, recall12, score12, support12 = metrics.precision_recall_fscore_support(Y_test,resultsY_12 )\n",
    "precision22, recall22, score22, support22 = metrics.precision_recall_fscore_support(Y_test,resultsY_22 )\n",
    "accuracy12 = metrics.accuracy_score(Y_test,resultsY_12)\n",
    "accuracy22 = metrics.accuracy_score(Y_test,resultsY_22)\n",
    "confusionmatrix12=metrics.confusion_matrix(Y_test,resultsY_12)\n",
    "confusionmatrix22=metrics.confusion_matrix(Y_test,resultsY_22)\n",
    "\n",
    "precision13, recall13, score13, support13 = metrics.precision_recall_fscore_support(Y_test,resultsY_13 )\n",
    "precision23, recall23, score23, support23 = metrics.precision_recall_fscore_support(Y_test,resultsY_23 )\n",
    "accuracy13 = metrics.accuracy_score(Y_test,resultsY_13)\n",
    "accuracy23 = metrics.accuracy_score(Y_test,resultsY_23)\n",
    "confusionmatrix13=metrics.confusion_matrix(Y_test,resultsY_13)\n",
    "confusionmatrix23=metrics.confusion_matrix(Y_test,resultsY_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "RESULTS Decision Tree Classifier - Model on Gini impurity \n",
      "__________________________________________\n",
      "BINARY\n",
      "Accuracy 0.686666666667\n",
      "F1_score [ 0.67857143  0.72727273  0.66      ]\n",
      "COUNT\n",
      "Accuracy 0.66\n",
      "F1_score [ 0.67857143  0.68131868  0.6185567 ]\n",
      "TFIDF\n",
      "Accuracy 0.626666666667\n",
      "F1_score [ 0.65454545  0.60759494  0.61261261]\n",
      "__________________________________________\n",
      "RESULTS Decision Tree Classifier  - Model on information gain (Entropy)\n",
      "__________________________________________\n",
      "BINARY\n",
      "Accuracy 0.666666666667\n",
      "F1_score [ 0.67272727  0.69565217  0.63265306]\n",
      "COUNT\n",
      "Accuracy 0.66\n",
      "F1_score [ 0.66666667  0.6741573   0.63917526]\n",
      "TFIDF\n",
      "Accuracy 0.606666666667\n",
      "F1_score [ 0.64150943  0.57894737  0.59322034]\n",
      "__________________________________________\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('__________________________________________') \n",
    "print('RESULTS Decision Tree Classifier - Model on Gini impurity ') \n",
    "print('__________________________________________') \n",
    "\n",
    "print ('BINARY')\n",
    "print('Accuracy', accuracy11)\n",
    "print('F1_score', score11)\n",
    "print ('COUNT')\n",
    "print('Accuracy', accuracy12)\n",
    "print('F1_score', score12)\n",
    "print ('TFIDF')\n",
    "print('Accuracy', accuracy13)\n",
    "print('F1_score', score13)\n",
    "print('__________________________________________') \n",
    "#print('__________________________________________') \n",
    "print('RESULTS Decision Tree Classifier  - Model on information gain (Entropy)') \n",
    "print('__________________________________________') \n",
    "print ('BINARY')\n",
    "print('Accuracy', accuracy21)\n",
    "print('F1_score', score21)\n",
    "print ('COUNT')\n",
    "print('Accuracy', accuracy22)\n",
    "print('F1_score', score22)\n",
    "print ('TFIDF')\n",
    "print('Accuracy', accuracy23)\n",
    "print('F1_score', score23)\n",
    "print('__________________________________________') \n",
    "print('__________________________________________') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Classifiers.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## =============================================================================\n",
    "## #Naive Bqyes using Multinomial or Gaussian Classifiers\n",
    "## =============================================================================\n",
    "\n",
    "print('Training Naive Classifiers.....')\n",
    "#models _1: binary vectorizers, _2: Count _3:Tfidf\n",
    " \n",
    "multi1_1 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "multi1_2 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "multi1_3 = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "#gaussian2_1 = GaussianNB()\n",
    "#gaussian2_2 = GaussianNB()\n",
    "#gaussian2_3 = GaussianNB()\n",
    "\n",
    "multi1_1.fit(train_vectors1,Y_train)\n",
    "#gaussian2_1.fit(train_vectors1,Y_train)\n",
    "\n",
    "multi1_2.fit(train_vectors2,Y_train)\n",
    "#gaussian2_2.fit(train_vectors2,Y_train)\n",
    "\n",
    "multi1_3.fit(train_vectors3,Y_train)\n",
    "#gaussian2_3.fit(train_vectors3,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_scoresMutli_1 [0.62626898 0.653367   0.7222257  0.63592446 0.62525391]\n",
      "cv_scoresMutli_3 [0.65050677 0.63958669 0.707114   0.63592446 0.61091234]\n",
      "cv_scoresMutli_3 [0.74104789 0.56260433 0.63960114 0.57908497 0.65099038]\n"
     ]
    }
   ],
   "source": [
    "#Run cross-validation to test split for each model vectorizer pair\n",
    "cv_scoresMutli_1 = cross_val_score(multi1_1, train_vectors1, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresMutli_2= cross_val_score(multi1_2, train_vectors2, Y_train, cv=5,scoring='f1_macro')\n",
    "cv_scoresMutli_3 = cross_val_score(multi1_3, train_vectors3, Y_train, cv=5,scoring='f1_macro')\n",
    "\n",
    "\n",
    "print('cv_scoresMutli_1', cv_scoresMutli_1)\n",
    "print('cv_scoresMutli_3', cv_scoresMutli_2)\n",
    "print('cv_scoresMutli_3', cv_scoresMutli_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Naïve Bayes Classifier Accuracy.....\n"
     ]
    }
   ],
   "source": [
    "#Test the data\n",
    "print('Testing Naïve Bayes Classifier Accuracy.....')\n",
    "\n",
    "resultsY_11 = multi1_1.predict(test_vectors1)\n",
    "\n",
    "resultsY_12 = multi1_2.predict(test_vectors2)\n",
    "\n",
    "\n",
    "resultsY_13 = multi1_3.predict(test_vectors3)\n",
    "\n",
    "precision11, recall11, score11, support11 = metrics.precision_recall_fscore_support(Y_test,resultsY_11 )\n",
    "accuracy11 = metrics.accuracy_score(Y_test,resultsY_11)\n",
    "confusionmatrix11=metrics.confusion_matrix(Y_test,resultsY_11)\n",
    "\n",
    "\n",
    "precision12, recall12, score12, support12 = metrics.precision_recall_fscore_support(Y_test,resultsY_12 )\n",
    "accuracy12 = metrics.accuracy_score(Y_test,resultsY_12)\n",
    "confusionmatrix12=metrics.confusion_matrix(Y_test,resultsY_12)\n",
    "\n",
    "\n",
    "precision13, recall13, score13, support13 = metrics.precision_recall_fscore_support(Y_test,resultsY_13 )\n",
    "accuracy13 = metrics.accuracy_score(Y_test,resultsY_13)\n",
    "confusionmatrix13=metrics.confusion_matrix(Y_test,resultsY_13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "RESULTS Decision Tree Classifier - Model on Gini impurity \n",
      "__________________________________________\n",
      "BINARY\n",
      "Accuracy 0.7466666666666667\n",
      "F1_score [0.8        0.68292683 0.74336283]\n",
      "COUNT\n",
      "Accuracy 0.76\n",
      "F1_score [0.81132075 0.69135802 0.76106195]\n",
      "TFIDF\n",
      "Accuracy 0.7333333333333333\n",
      "F1_score [0.7706422  0.64788732 0.75      ]\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('__________________________________________') \n",
    "print('RESULTS Decision Tree Classifier - Model on Gini impurity ') \n",
    "print('__________________________________________') \n",
    "\n",
    "print ('BINARY')\n",
    "print('Accuracy', accuracy11)\n",
    "print('F1_score', score11)\n",
    "print ('COUNT')\n",
    "print('Accuracy', accuracy12)\n",
    "print('F1_score', score12)\n",
    "print ('TFIDF')\n",
    "print('Accuracy', accuracy13)\n",
    "print('F1_score', score13)\n",
    "print('__________________________________________') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS - VADER\n",
      "__________________________________________\n",
      "Accuracy 0.686666666667\n",
      "F1_score [ 0.68131868  0.64444444  0.72268908]\n",
      "__________________________________________\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "## =============================================================================\n",
    "## Apply Vader analysis on test set\n",
    "## ============================================================================= \n",
    "vader_scores_test=[]\n",
    "vader_scores_test_mod=[]\n",
    "analyzer = SentimentIntensityAnalyzer() # create a vader analyzer\n",
    "for tweet in X_test:\n",
    "    vader_scores_test.append(analyzer.polarity_scores(tweet)['compound'])#apply analyser to tweet set\n",
    "\n",
    "#convert scores to positive, neg and neutral, 0.3 was chosen due to the compound score being between 1 and neg 1\n",
    "for score in vader_scores_test:\n",
    "    if float(score)>=0.30: #positive sentiment\n",
    "        vader_scores_test_mod.append(4)\n",
    "    elif float(score)<=-0.30: #negativesentiment\n",
    "        vader_scores_test_mod.append(0)\n",
    "    else: \n",
    "        vader_scores_test_mod.append(2)\n",
    "    \n",
    "precision_vader, recall_vader, score_vader, support_vader = metrics.precision_recall_fscore_support( Y_test,vader_scores_test_mod)\n",
    "accuracy_vader = metrics.accuracy_score( Y_test,vader_scores_test_mod)\n",
    "confusionmatrix_vader=metrics.confusion_matrix( Y_test,vader_scores_test_mod)\n",
    "\n",
    "print('RESULTS - VADER') \n",
    "print('__________________________________________') \n",
    "print('Accuracy', accuracy_vader)\n",
    "print('F1_score',score_vader)\n",
    "print('__________________________________________') \n",
    "print('__________________________________________') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "### =============================================================================\n",
    "### Apply best model to twitter data and print a selection of results\n",
    "### =============================================================================\n",
    "# NOTE: Current best model is Model1_3 One vs rest tdif vectorizer, with current parameters\n",
    "file = open('olympics.json','r')\n",
    "text = file.read()\n",
    "split_text = text.split('\\n')\n",
    "split_text.pop() #Last elemnt is empty because of the split\n",
    "\n",
    "tweet_test = [json.loads(data)['text'] for data in split_text]\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "tweet_vector = vectorizer3.transform(tweet_test)\n",
    "tweet_prediction = model1_3.predict(tweet_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "___________PREDICTIONS_______________\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "NEUTRAL\n",
      "['So glad the #LadiesFreeSkate is tomorrow! Thought I missed it! \\n#Rooting4Evgenia 💜⛸\\n#Figureskating \\n#PyeongChang2018 \\n#Olympics', 'Lindsey Vonn wins her second bronze for downhill, Tiger Woods was her first downhill bronze. #Olympics… https://t.co/F6LSFO0bQR']\n",
      "__________________________________________\n",
      "POSITIVE\n",
      "[\"A king's ransom for one clean short program please....#Olympics\", 'Fun and games in the parking lot.\\n@tjmn93 @lisakayemn93 @DocBartender\\n#mnwx #OnlyinMN #Olympics https://t.co/dk6ugitV5t']\n",
      "__________________________________________\n",
      "NEGATIVE\n",
      "['USA GOLD MEDAL!!! 🇺🇸\\U0001f947🇺🇸\\U0001f947🇺🇸\\U0001f947🇺🇸\\U0001f947 #USAHOCKEY #OLYMPICS', \"Why do you watch #Olympics ? I'm curious because other channels don't seem to want to compete with the Olympics.\"]\n"
     ]
    }
   ],
   "source": [
    "#print 2 random tweets of each predicted class \n",
    "neutral_ind=(tweet_prediction==2).nonzero()[0]\n",
    "pos_ind=(tweet_prediction==4).nonzero()[0]\n",
    "neg_ind=(tweet_prediction==0).nonzero()[0]\n",
    "\n",
    "tweet_test=numpy.asarray(tweet_test)\n",
    "neutral=tweet_test[neutral_ind]\n",
    "pos=tweet_test[pos_ind]\n",
    "neg=tweet_test[neg_ind]\n",
    "tweet_test=tweet_test.tolist()\n",
    "\n",
    "rand_smpl_neutral = [ neutral[i] for i in sorted(random.sample(range(len(neutral)), 2)) ]\n",
    "rand_smpl_pos = [ pos[i] for i in sorted(random.sample(range(len(pos)), 2)) ]\n",
    "rand_smpl_neg = [ neg[i] for i in sorted(random.sample(range(len(neg)), 2)) ]\n",
    "print('__________________________________________') \n",
    "print('___________PREDICTIONS_______________') \n",
    "print('__________________________________________') \n",
    "\n",
    "print('__________________________________________') \n",
    "print('NEUTRAL')\n",
    "print(rand_smpl_neutral)\n",
    "print('__________________________________________') \n",
    "print('POSITIVE')\n",
    "print(rand_smpl_pos)\n",
    "print('__________________________________________') \n",
    "print('NEGATIVE')\n",
    "print(rand_smpl_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## =============================================================================\n",
    "## Apply Vader analysis and compare scores, <=-0.8: #negativesentiment >=0.8: #positive sentiment\n",
    "## ============================================================================= \n",
    "vader_scores_orig=[]\n",
    "vader_scores=[]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer() # create a vader analyzer\n",
    "for tweet in tweet_test:\n",
    "    vader_scores_orig.append(analyzer.polarity_scores(tweet)['compound'])#apply analyser to tweet set\n",
    "\n",
    "#convert scores to positive, neg and neutral \n",
    "for score in vader_scores_orig:\n",
    "    if float(score)>=0.8: #positive sentiment\n",
    "        vader_scores.append(4)\n",
    "    elif float(score)<=-0.8: #negativesentiment\n",
    "        vader_scores.append(0)\n",
    "    else: \n",
    "        vader_scores.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET- Model-Vader comparison\n",
      "__________________________________________\n",
      "Accuracy 0.1092\n",
      "F1_score [ 0.02080344  0.10504634  0.2223165 ]\n",
      "__________________________________________\n",
      "__________________________________________\n"
     ]
    }
   ],
   "source": [
    "## =============================================================================\n",
    "## Analyse results\n",
    "## ============================================================================= \n",
    "\n",
    "precisionT, recallT, scoreT, supportT = metrics.precision_recall_fscore_support( vader_scores,tweet_prediction )\n",
    "accuracyT = metrics.accuracy_score( vader_scores,tweet_prediction)\n",
    "confusionmatrixT=metrics.confusion_matrix( vader_scores,tweet_prediction)\n",
    "\n",
    "print('TWEET- Model-Vader comparison') \n",
    "print('__________________________________________') \n",
    "print('Accuracy', accuracyT)\n",
    "print('F1_score', scoreT)\n",
    "print('__________________________________________') \n",
    "print('__________________________________________') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
